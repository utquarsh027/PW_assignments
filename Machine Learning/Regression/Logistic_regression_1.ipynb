{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4cb75e",
   "metadata": {},
   "source": [
    "#### Q1. Explain the difference between linear regression and logistic regression models. Provide an example of a scenario where logistic regression would be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca08f70f",
   "metadata": {},
   "source": [
    "**Linear Regression** and **Logistic Regression** are both types of regression models used in machine learning, but they serve different purposes and have distinct characteristics:\n",
    "\n",
    "**Linear Regression:**\n",
    "\n",
    "1. **Purpose:** Linear regression is used for predicting a continuous numeric output or target variable. It models the relationship between the independent variables (features) and the continuous outcome variable by fitting a linear equation to the data.\n",
    "\n",
    "2. **Output:** The output of linear regression is a continuous value that can range from negative infinity to positive infinity.\n",
    "\n",
    "3. **Example:** Suppose you want to predict a person's salary based on their years of experience. In this case, you are trying to estimate a numeric value (salary), making linear regression an appropriate choice.\n",
    "\n",
    "**Logistic Regression:**\n",
    "\n",
    "1. **Purpose:** Logistic regression is used for classification tasks, where the goal is to predict a binary outcome or assign an observation to one of two classes or categories.\n",
    "\n",
    "2. **Output:** The output of logistic regression is a probability score between 0 and 1, which represents the probability that an observation belongs to one of the classes. It uses the logistic (sigmoid) function to map the linear combination of features to the probability.\n",
    "\n",
    "3. **Example:** Consider a scenario where you want to predict whether an email is spam or not based on features like the subject line, sender, and content of the email. In this case, logistic regression is suitable because it deals with binary classification, determining whether an email belongs to the \"spam\" or \"not spam\" class.\n",
    "\n",
    "**Scenario Where Logistic Regression is More Appropriate:**\n",
    "\n",
    "Logistic regression is more appropriate when you are dealing with classification problems or scenarios where you need to make a binary decision. Here are some examples where logistic regression is commonly used:\n",
    "\n",
    "1. **Spam Detection:** As mentioned earlier, classifying emails as spam or not spam is a classic example of binary classification, where logistic regression can be used to predict the probability of an email being spam.\n",
    "\n",
    "2. **Customer Churn Prediction:** Predicting whether a customer will churn (leave) a service or remain as a customer is another common application of logistic regression. The outcome is binary: churn or no churn.\n",
    "\n",
    "3. **Medical Diagnosis:** In medical applications, logistic regression can be used to predict the likelihood of a patient having a specific condition (e.g., disease or not disease) based on various medical features and test results.\n",
    "\n",
    "4. **Credit Risk Assessment:** Banks and financial institutions often use logistic regression to assess the credit risk of applicants. The goal is to predict whether a borrower is likely to default on a loan (high risk) or not (low risk).\n",
    "\n",
    "5. **Employee Attrition:** Predicting whether an employee is likely to leave a company (attrition) or stay can be framed as a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529682a9",
   "metadata": {},
   "source": [
    "#### Q2. What is the cost function used in logistic regression, and how is it optimized?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e90a28",
   "metadata": {},
   "source": [
    "In logistic regression, the cost function, often referred to as the **logistic loss** or **cross-entropy loss**, is used to measure the error or difference between the predicted probabilities and the actual binary outcomes of a classification problem. The cost function for logistic regression is as follows:\n",
    "\n",
    "**Binary Cross-Entropy Loss (Log Loss):**\n",
    "\n",
    "For a single data point (instance), the binary cross-entropy loss is defined as:\n",
    "\n",
    "\\[J(y, \\hat{y}) = - [y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})]\\]\n",
    "\n",
    "Where:\n",
    "- \\(J(y, \\hat{y})\\) is the cost associated with predicting \\(\\hat{y}\\) when the true label is \\(y\\).\n",
    "- \\(y\\) is the actual binary class label (0 or 1).\n",
    "- \\(\\hat{y}\\) is the predicted probability that the instance belongs to class 1 (the positive class).\n",
    "\n",
    "For a dataset with multiple instances, the overall cost function is the average of these individual losses:\n",
    "\n",
    "\\[J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} [-y^{(i)} \\log(\\hat{y}^{(i)}) - (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})]\\]\n",
    "\n",
    "Where:\n",
    "- \\(J(\\theta)\\) is the total cost function over the entire dataset.\n",
    "- \\(m\\) is the number of training examples.\n",
    "- \\(y^{(i)}\\) is the actual label for the \\(i\\)-th training example.\n",
    "- \\(\\hat{y}^{(i)}\\) is the predicted probability for the \\(i\\)-th training example.\n",
    "- \\(\\theta\\) represents the model parameters (coefficients).\n",
    "\n",
    "**Optimization:**\n",
    "\n",
    "The goal of logistic regression is to find the model parameters (\\(\\theta\\)) that minimize the cost function \\(J(\\theta)\\). This is typically done using optimization techniques. The most commonly used optimization algorithm for logistic regression is **Gradient Descent**:\n",
    "\n",
    "**Gradient Descent:**\n",
    "Gradient Descent is an iterative optimization algorithm that updates the model parameters (\\(\\theta\\)) in the direction of steepest decrease of the cost function \\(J(\\theta)\\). The update rule for a single parameter \\(\\theta_j\\) in each iteration is as follows:\n",
    "\n",
    "\\[\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}\\]\n",
    "\n",
    "Where:\n",
    "- \\(\\alpha\\) is the learning rate, a hyperparameter that determines the step size in each iteration.\n",
    "- \\(\\frac{\\partial J(\\theta)}{\\partial \\theta_j}\\) is the partial derivative of the cost function with respect to the parameter \\(\\theta_j\\).\n",
    "\n",
    "The process continues until convergence, where the cost function reaches a minimum or a predefined stopping criterion is met.\n",
    "\n",
    "To compute the gradient \\(\\frac{\\partial J(\\theta)}{\\partial \\theta_j}\\), you need to calculate the derivative of the cost function with respect to each parameter \\(\\theta_j\\). Fortunately, for logistic regression, these derivatives can be computed analytically, resulting in a closed-form solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd920c",
   "metadata": {},
   "source": [
    "#### Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a8b012",
   "metadata": {},
   "source": [
    "\n",
    "Regularization in logistic regression is a technique used to prevent overfitting, which occurs when a model fits the training data too closely, capturing noise and making it less generalizable to unseen data. Regularization introduces a penalty term to the logistic regression cost function, discouraging the model from assigning excessively large weights to individual features. This helps in creating a simpler and more robust model that generalizes better to new data.\n",
    "\n",
    "There are two common types of regularization used in logistic regression:\n",
    "\n",
    "L1 Regularization (Lasso Regularization):\n",
    "\n",
    "In L1 regularization, a penalty term is added to the cost function, which is proportional to the absolute values of the model's coefficients (weights).\n",
    "The L1 regularization term is represented as \n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "∣\n",
    "�\n",
    "�\n",
    "∣\n",
    "λ∑ \n",
    "j=1\n",
    "n\n",
    "​\n",
    " ∣w \n",
    "j\n",
    "​\n",
    " ∣, where \n",
    "�\n",
    "�\n",
    "w \n",
    "j\n",
    "​\n",
    "  is the weight of feature \n",
    "�\n",
    "j, and \n",
    "�\n",
    "λ is the regularization parameter, controlling the strength of the regularization.\n",
    "L1 regularization encourages sparsity in the model, meaning it tends to set some feature weights to exactly zero. As a result, it performs feature selection by automatically selecting the most relevant features, effectively reducing the model's complexity.\n",
    "L2 Regularization (Ridge Regularization):\n",
    "\n",
    "In L2 regularization, a penalty term is added to the cost function, which is proportional to the square of the model's coefficients (weights).\n",
    "The L2 regularization term is represented as \n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "�\n",
    "�\n",
    "2\n",
    "λ∑ \n",
    "j=1\n",
    "n\n",
    "​\n",
    " w \n",
    "j\n",
    "2\n",
    "​\n",
    " .\n",
    "L2 regularization encourages all feature weights to be small but rarely exactly zero. It helps in preventing extreme weight values and ensures that all features contribute to the prediction to some extent.\n",
    "The choice between L1 and L2 regularization depends on the specific problem and the desired behavior of the model:\n",
    "\n",
    "L1 regularization is often preferred when there is a suspicion that only a subset of features is relevant for the prediction, and you want automatic feature selection. It helps reduce the dimensionality of the model.\n",
    "\n",
    "L2 regularization is more commonly used as a general-purpose regularization technique. It helps prevent overfitting by penalizing large weights without forcing them to be exactly zero. This can lead to a smoother and more stable model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547aeeea",
   "metadata": {},
   "source": [
    "#### Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770349b7",
   "metadata": {},
   "source": [
    "\n",
    "The ROC (Receiver Operating Characteristic) curve is a graphical representation used to evaluate the performance of a binary classification model, such as logistic regression. It plots the trade-off between the model's true positive rate (sensitivity) and its false positive rate (1 - specificity) across various classification thresholds. The ROC curve is a valuable tool for assessing the model's ability to discriminate between the positive and negative classes and for selecting an appropriate threshold based on specific application requirements.\n",
    "\n",
    "Here's how the ROC curve is created and used to evaluate a logistic regression model:\n",
    "\n",
    "Calculate Probabilities: The logistic regression model produces probability scores for each data point, representing the likelihood that the data point belongs to the positive class (class 1). These probabilities typically range from 0 to 1.\n",
    "\n",
    "Threshold Selection: To make binary predictions, you need to choose a classification threshold (e.g., 0.5). Data points with probabilities above the threshold are classified as positive, and those below the threshold are classified as negative. The ROC curve evaluates the model's performance at different thresholds.\n",
    "\n",
    "True Positive Rate (Sensitivity): The true positive rate (TPR), also known as sensitivity or recall, is the proportion of true positives (correctly predicted positive cases) among all actual positive cases. It is calculated as:\n",
    "\n",
    "�\n",
    "�\n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "TPR= \n",
    "TP+FN\n",
    "TP\n",
    "​\n",
    " \n",
    "\n",
    "�\n",
    "�\n",
    "TP is the number of true positives.\n",
    "�\n",
    "�\n",
    "FN is the number of false negatives.\n",
    "False Positive Rate (1 - Specificity): The false positive rate (FPR), which is equal to \n",
    "1\n",
    "−\n",
    "specificity\n",
    "1−specificity, is the proportion of false positives (incorrectly predicted positive cases) among all actual negative cases. It is calculated as:\n",
    "\n",
    "�\n",
    "�\n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "FPR= \n",
    "TN+FP\n",
    "FP\n",
    "​\n",
    " \n",
    "\n",
    "�\n",
    "�\n",
    "TN is the number of true negatives.\n",
    "�\n",
    "�\n",
    "FP is the number of false positives.\n",
    "ROC Curve Construction: The ROC curve is created by plotting the TPR (sensitivity) on the y-axis and the FPR (1 - specificity) on the x-axis as the classification threshold varies. Each point on the ROC curve represents the model's performance at a different threshold.\n",
    "\n",
    "AUC-ROC Score: The Area Under the ROC Curve (AUC-ROC) is a quantitative measure of the model's overall performance. It quantifies the ability of the model to distinguish between the positive and negative classes. A model with an AUC-ROC score of 0.5 performs no better than random guessing, while a perfect model has an AUC-ROC score of 1.0.\n",
    "\n",
    "An AUC-ROC score of 0.5 indicates that the model is no better than random chance in distinguishing between the two classes.\n",
    "An AUC-ROC score greater than 0.5 suggests that the model has some discriminatory power, with a higher score indicating better performance.\n",
    "An AUC-ROC score of 1.0 indicates perfect discrimination, where the model correctly ranks all positive cases higher than negative cases.\n",
    "Threshold Selection: Depending on the specific requirements of your application, you can choose an appropriate classification threshold by examining the ROC curve. If minimizing false positives is critical, you might choose a threshold that corresponds to a specific point on the curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019aa724",
   "metadata": {},
   "source": [
    "#### Q5. What are some common techniques for feature selection in logistic regression? How do these techniques help improve the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2f5f99",
   "metadata": {},
   "source": [
    "\n",
    "Feature selection in logistic regression is the process of choosing a subset of the most relevant features (independent variables) from the original set of features to improve the model's performance, reduce overfitting, and enhance interpretability. Here are some common techniques for feature selection in logistic regression and how they help improve the model's performance:\n",
    "\n",
    "Univariate Feature Selection:\n",
    "\n",
    "Univariate feature selection methods evaluate each feature individually in relation to the target variable. Common techniques include chi-squared test, ANOVA F-test, and mutual information.\n",
    "Features with the highest statistical significance (e.g., the lowest p-values) are selected.\n",
    "These methods help identify features that have a strong influence on the target variable and are likely to improve model performance.\n",
    "\n",
    "Feature Importance from Tree-Based Models:\n",
    "\n",
    "Tree-based models like Random Forest and XGBoost can estimate feature importance scores based on how often features are used for splitting nodes in the trees.\n",
    "Features with higher importance scores are considered more informative and can be selected.\n",
    "These methods help in identifying features that contribute significantly to the model's predictive power.\n",
    "\n",
    "Recursive Feature Elimination (RFE):\n",
    "\n",
    "RFE is an iterative method that starts with all features and progressively removes the least important ones based on the model's performance.\n",
    "It continues until a desired number of features or a predefined stopping criterion is reached.\n",
    "RFE helps to identify a compact set of features that still provides good predictive performance.\n",
    "\n",
    "L1 Regularization (Lasso Regression):\n",
    "\n",
    "L1 regularization encourages sparsity in the model, meaning it tends to set some feature weights to exactly zero.\n",
    "Features with non-zero weights are selected, effectively performing automatic feature selection.\n",
    "Lasso regression helps in identifying and utilizing only the most relevant features.\n",
    "\n",
    "Correlation-based Feature Selection:\n",
    "\n",
    "Features that are highly correlated with the target variable but not highly correlated with other features are selected.\n",
    "This method helps in identifying features with a direct impact on the target while avoiding multicollinearity.\n",
    "Recursive Feature Addition (RFA):\n",
    "\n",
    "RFA is the opposite of RFE. It starts with an empty set of features and adds the most important ones based on the model's performance.\n",
    "It continues until a desired number of features or a predefined stopping criterion is reached.\n",
    "RFA helps identify the most relevant features incrementally.\n",
    "\n",
    "Embedded Methods:\n",
    "\n",
    "Some machine learning algorithms, including logistic regression, have built-in mechanisms for feature selection.\n",
    "For example, logistic regression with L1 regularization (Lasso) directly performs feature selection by assigning zero coefficients to unimportant features.\n",
    "Other algorithms may use feature importance scores or pruning techniques during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13da9fd4",
   "metadata": {},
   "source": [
    "#### Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing with class imbalance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605226c4",
   "metadata": {},
   "source": [
    "\n",
    "Handling imbalanced datasets in logistic regression is essential because when one class significantly outnumbers the other, the model may become biased towards the majority class. This can result in poor predictive performance, as the model may struggle to correctly classify the minority class. Here are some strategies for dealing with class imbalance in logistic regression:\n",
    "\n",
    "Resampling Techniques:\n",
    "\n",
    "Oversampling: Increase the number of instances in the minority class by duplicating existing samples or generating synthetic data points. Popular oversampling methods include SMOTE (Synthetic Minority Over-sampling Technique) and ADASYN (Adaptive Synthetic Sampling).\n",
    "\n",
    "Undersampling: Reduce the number of instances in the majority class by randomly removing some samples. Undersampling methods include random undersampling and Tomek links.\n",
    "\n",
    "Combined Sampling: Combine oversampling of the minority class and undersampling of the majority class to create a balanced dataset.\n",
    "\n",
    "Generate Synthetic Data:\n",
    "\n",
    "Use generative models like Variational Autoencoders (VAEs) or Generative Adversarial Networks (GANs) to generate synthetic data for the minority class. These models can create new, realistic instances of the minority class, helping to balance the dataset.\n",
    "Different Evaluation Metrics:\n",
    "\n",
    "Instead of using accuracy as the evaluation metric, consider metrics that are more suitable for imbalanced datasets, such as precision, recall, F1-score, or the area under the ROC curve (AUC-ROC). These metrics provide a better assessment of model performance when class distribution is skewed.\n",
    "Cost-Sensitive Learning:\n",
    "\n",
    "Assign different misclassification costs to the classes. In imbalanced datasets, you can assign a higher cost to misclassifying the minority class, encouraging the model to focus on correctly classifying the minority class.\n",
    "Threshold Adjustment:\n",
    "\n",
    "By default, logistic regression uses a threshold of 0.5 to make binary predictions. Adjusting this threshold can influence the trade-off between precision and recall. If you prioritize recall for the minority class, you can lower the threshold, and if you prioritize precision, you can raise it.\n",
    "Ensemble Methods:\n",
    "\n",
    "Use ensemble techniques like Random Forest or Gradient Boosting with class weights or resampling techniques. These models can handle class imbalance more effectively than standalone logistic regression.\n",
    "Anomaly Detection:\n",
    "\n",
    "Treat the minority class as anomalies and use anomaly detection techniques to identify them. Techniques like Isolation Forest and One-Class SVM can be applied to identify instances of the minority class.\n",
    "Collect More Data:\n",
    "\n",
    "If feasible, collect additional data for the minority class to balance the dataset naturally. More data can help the model learn the minority class better.\n",
    "Penalized Models:\n",
    "\n",
    "Use penalized models like penalized logistic regression (e.g., with L1 or L2 regularization) that assign penalties to the model coefficients based on class misclassification.\n",
    "Change the Decision Threshold During Inference:\n",
    "\n",
    "When making predictions on new data, you can adjust the decision threshold dynamically based on your specific requirements and the consequences of false positives and false negatives.\n",
    "Hybrid Approaches:\n",
    "\n",
    "Combine multiple strategies mentioned above to address class imbalance effectively. For example, you can oversample the minority class, use cost-sensitive learning, and employ ensemble methods together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8f6c08",
   "metadata": {},
   "source": [
    "#### Q7. Can you discuss some common issues and challenges that may arise when implementing logistic regression, and how they can be addressed? For example, what can be done if there is multicollinearity among the independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919becfe",
   "metadata": {},
   "source": [
    "Implementing logistic regression, like any machine learning technique, can come with several challenges and issues. Here are some common challenges and strategies to address them:\n",
    "\n",
    "1. **Multicollinearity:**\n",
    "   \n",
    "   - **Issue:** Multicollinearity occurs when two or more independent variables in the logistic regression model are highly correlated with each other. This can lead to unstable coefficient estimates and difficulty in interpreting the effects of individual predictors.\n",
    "   \n",
    "   - **Solution:** To address multicollinearity:\n",
    "     - Identify the correlated variables using correlation matrices or variance inflation factor (VIF) analysis.\n",
    "     - Consider removing one of the highly correlated variables or using dimensionality reduction techniques like Principal Component Analysis (PCA).\n",
    "     - Regularization techniques like L2 (Ridge) regularization can also help mitigate multicollinearity by shrinking coefficients.\n",
    "\n",
    "2. **Imbalanced Data:**\n",
    "   \n",
    "   - **Issue:** When one class is significantly more prevalent than the other, logistic regression may produce biased results. It may have difficulty predicting the minority class.\n",
    "   \n",
    "   - **Solution:** Refer to the strategies mentioned in the previous answer for handling imbalanced data, including resampling, cost-sensitive learning, and appropriate performance metrics.\n",
    "\n",
    "3. **Non-Linearity:**\n",
    "   \n",
    "   - **Issue:** Logistic regression assumes a linear relationship between the independent variables and the log-odds of the dependent variable. If the relationship is non-linear, the model may not perform well.\n",
    "   \n",
    "   - **Solution:** To handle non-linearity:\n",
    "     - Transform variables (e.g., using polynomials or splines) to capture non-linear patterns.\n",
    "     - Consider using more complex models like decision trees, random forests, or neural networks if non-linearity is a significant concern.\n",
    "\n",
    "4. **Overfitting:**\n",
    "   \n",
    "   - **Issue:** Overfitting occurs when the model fits the training data too closely, capturing noise and performing poorly on new, unseen data.\n",
    "   \n",
    "   - **Solution:** Prevent overfitting by:\n",
    "     - Regularization: Use L1 or L2 regularization to penalize large coefficients.\n",
    "     - Cross-validation: Employ cross-validation techniques to assess the model's performance on unseen data and select hyperparameters.\n",
    "     - Feature selection: Remove irrelevant or noisy features.\n",
    "     - Collect more data if possible to improve model generalization.\n",
    "\n",
    "5. **Missing Data:**\n",
    "   \n",
    "   - **Issue:** Logistic regression cannot handle missing data directly. Missing values in the independent variables need to be addressed.\n",
    "   \n",
    "   - **Solution:** Deal with missing data by:\n",
    "     - Imputing missing values with means, medians, or a predictive model.\n",
    "     - Using techniques like listwise deletion (removing rows with missing data) when appropriate.\n",
    "     - Considering specialized imputation methods if data is missing non-randomly.\n",
    "\n",
    "6. **Outliers:**\n",
    "   \n",
    "   - **Issue:** Outliers can disproportionately influence the logistic regression model and lead to biased results.\n",
    "   \n",
    "   - **Solution:** Handle outliers by:\n",
    "     - Identifying and assessing outliers using visualization and statistical methods.\n",
    "     - Consider transforming or winsorizing the data to mitigate the impact of outliers.\n",
    "     - Use robust logistic regression techniques that are less sensitive to outliers.\n",
    "\n",
    "7. **Sample Size:**\n",
    "   \n",
    "   - **Issue:** Logistic regression models may require a sufficiently large sample size to provide reliable estimates.\n",
    "   \n",
    "   - **Solution:** Ensure an adequate sample size by:\n",
    "     - Conducting power analysis to determine the minimum sample size required.\n",
    "     - Employing resampling techniques like bootstrapping if increasing the sample size is not feasible.\n",
    "\n",
    "8. **Interactions and Non-Independence:**\n",
    "   \n",
    "   - **Issue:** Logistic regression assumes independence of observations, which may not hold in some cases. Additionally, interactions between variables may be important.\n",
    "   \n",
    "   - **Solution:** To address interactions and non-independence:\n",
    "     - Consider adding interaction terms to capture relationships between variables.\n",
    "     - Account for non-independence using appropriate modeling techniques like mixed-effects logistic regression if the data has hierarchical or clustered structures.\n",
    "\n",
    "Addressing these challenges often requires careful data preprocessing, feature engineering, model selection, and evaluation. It's crucial to thoroughly understand the characteristics of your data and the assumptions of logistic regression to effectively address these issues and build a robust predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b1686c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
