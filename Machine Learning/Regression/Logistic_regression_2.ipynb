{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "454fca35",
   "metadata": {},
   "source": [
    "#### Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5cae0f",
   "metadata": {},
   "source": [
    "Grid Search Cross-Validation (Grid Search CV) is a technique used in machine learning to systematically search for the optimal combination of hyperparameters for a given model. It is commonly used to fine-tune the hyperparameters of a machine learning algorithm to achieve the best possible performance on a specific task or dataset. Here's how it works and its purpose:\n",
    "\n",
    "**Purpose of Grid Search CV:**\n",
    "\n",
    "1. **Hyperparameter Tuning:** Machine learning algorithms often have hyperparameters that are not learned from the training data but need to be set before training. These hyperparameters can significantly affect the model's performance.\n",
    "\n",
    "2. **Optimizing Model Performance:** The goal of Grid Search CV is to find the hyperparameter values that result in the best model performance, typically measured by a chosen evaluation metric (e.g., accuracy, F1-score, AUC-ROC).\n",
    "\n",
    "**How Grid Search CV Works:**\n",
    "\n",
    "Grid Search CV works by exhaustively trying all possible combinations of hyperparameter values from a predefined grid or set. Here are the key steps:\n",
    "\n",
    "1. **Define Hyperparameter Grid:**\n",
    "   \n",
    "   - You specify a range of values or a set of choices for each hyperparameter you want to tune. For example, you might define a grid for the learning rate, the number of trees in a random forest, or the regularization strength in a logistic regression model.\n",
    "\n",
    "2. **Cross-Validation:**\n",
    "\n",
    "   - Grid Search CV uses a cross-validation technique, often k-fold cross-validation, to evaluate the model's performance for each combination of hyperparameters. In k-fold cross-validation, the data is divided into k subsets (folds). The model is trained on k-1 folds and evaluated on the remaining fold, repeating this process k times to ensure each fold is used as the validation set exactly once.\n",
    "\n",
    "3. **Model Training and Evaluation:**\n",
    "\n",
    "   - For each hyperparameter combination, the model is trained on the training data (k-1 folds) using the specified hyperparameters.\n",
    "   - Then, it is evaluated on the validation fold, and a performance metric (e.g., accuracy) is computed.\n",
    "   \n",
    "4. **Hyperparameter Selection:**\n",
    "\n",
    "   - The combination of hyperparameters that results in the best performance metric is selected as the optimal set of hyperparameters.\n",
    "\n",
    "5. **Final Model Training:**\n",
    "\n",
    "   - Once the optimal hyperparameters are identified, the final model is trained on the entire training dataset using these hyperparameters.\n",
    "\n",
    "6. **Model Evaluation:**\n",
    "\n",
    "   - The final model's performance is evaluated on a separate test dataset (not used during hyperparameter tuning) to estimate how well it will perform on unseen data.\n",
    "\n",
    "Grid Search CV systematically explores the hyperparameter space, helping you find the best hyperparameters for your model while avoiding manual trial-and-error. It automates the process of hyperparameter tuning and is especially useful when there are multiple hyperparameters to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b418dbac",
   "metadata": {},
   "source": [
    "#### Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316cf7d6",
   "metadata": {},
   "source": [
    "**Grid Search CV** and **Randomized Search CV** are both techniques used for hyperparameter tuning in machine learning. They aim to find the best combination of hyperparameters to optimize a model's performance. However, they differ in their approach to exploring the hyperparameter space, and the choice between them depends on various factors.\n",
    "\n",
    "**Grid Search CV:**\n",
    "\n",
    "1. **Approach:** Grid Search CV performs an exhaustive search over a predefined grid of hyperparameters. It systematically explores all possible combinations of hyperparameter values within the specified ranges or choices.\n",
    "\n",
    "2. **Search Space:** It covers every combination of hyperparameter values in the grid, making it a deterministic and exhaustive approach.\n",
    "\n",
    "3. **Advantages:**\n",
    "   - Guaranteed to find the best combination if it exists within the search space.\n",
    "   - Provides a comprehensive view of the hyperparameter landscape.\n",
    "   - Simple to set up and interpret.\n",
    "\n",
    "4. **Disadvantages:**\n",
    "   - Computationally expensive, especially when the search space is large.\n",
    "   - May not be efficient when some hyperparameters have a much greater impact on performance than others.\n",
    "   - May not be suitable when computational resources are limited.\n",
    "\n",
    "**Randomized Search CV:**\n",
    "\n",
    "1. **Approach:** Randomized Search CV, as the name suggests, conducts a random search over the hyperparameter space. It randomly samples a specified number of combinations from the hyperparameter distributions or ranges.\n",
    "\n",
    "2. **Search Space:** It explores a subset of the hyperparameter space, which is randomly sampled according to user-defined distributions or ranges. It's a probabilistic approach.\n",
    "\n",
    "3. **Advantages:**\n",
    "   - More computationally efficient than Grid Search, especially when the search space is extensive.\n",
    "   - Allows focusing computational resources on the most important hyperparameters.\n",
    "   - Can discover good hyperparameter combinations faster than Grid Search.\n",
    "\n",
    "4. **Disadvantages:**\n",
    "   - There is no guarantee of finding the absolute best combination since it explores a random subset.\n",
    "   - The results may be less interpretable due to the random sampling.\n",
    "\n",
    "**When to Choose Grid Search CV vs. Randomized Search CV:**\n",
    "\n",
    "The choice between Grid Search CV and Randomized Search CV depends on various factors:\n",
    "\n",
    "1. **Computational Resources:**\n",
    "   - If computational resources are limited, Randomized Search is often preferred because it explores a smaller subset of the hyperparameter space.\n",
    "\n",
    "2. **Search Space Size:**\n",
    "   - For a small and manageable hyperparameter search space, Grid Search CV may be suitable since it exhaustively explores all combinations.\n",
    "\n",
    "3. **Hyperparameter Importance:**\n",
    "   - If you have prior knowledge that certain hyperparameters are more critical than others, you may want to use Randomized Search to focus on those key hyperparameters.\n",
    "\n",
    "4. **Efficiency vs. Exhaustiveness:**\n",
    "   - If you need a quick exploration of hyperparameters, Randomized Search CV is more efficient. However, if you require a comprehensive understanding of the entire hyperparameter landscape, Grid Search CV might be preferred.\n",
    "\n",
    "5. **Interpretability:**\n",
    "   - Grid Search CV provides a clear and interpretable view of the search space, making it easier to understand the impact of different hyperparameters on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0ba820",
   "metadata": {},
   "source": [
    "#### Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9d4eee",
   "metadata": {},
   "source": [
    "\n",
    "Data leakage in machine learning refers to a situation where information from outside the training dataset is inadvertently used to train a model or make predictions, leading to misleadingly good performance or erroneous results. Data leakage can significantly compromise the integrity and generalizability of a machine learning model. It is a problem because it can make a model appear more accurate than it actually is, and it can lead to poor performance on unseen data. Here's why data leakage is a problem and an example to illustrate it:\n",
    "\n",
    "Why Data Leakage Is a Problem:\n",
    "\n",
    "Overly Optimistic Performance: Data leakage can make a model seem highly accurate during training and cross-validation because it has access to information that it shouldn't have during inference. As a result, it may give a false impression of the model's effectiveness.\n",
    "\n",
    "Poor Generalization: Models trained with data leakage often fail to generalize to new, unseen data because they rely on patterns or information that are specific to the training dataset but not present in real-world scenarios.\n",
    "\n",
    "Unrealistic Expectations: Data leakage can lead to unrealistic expectations and overconfidence in a model's performance, which can have serious consequences in applications such as healthcare, finance, and safety-critical systems.\n",
    "\n",
    "Example of Data Leakage:\n",
    "\n",
    "Suppose you are building a credit risk model to predict whether a loan applicant is likely to default on a loan or not. You have a dataset containing historical loan data, including the applicant's income, credit score, employment history, and loan outcome (default or not default). In the dataset, there is a variable called \"outstanding debt,\" which represents the applicant's existing debt at the time of loan application.\n",
    "\n",
    "Data Leakage Scenario:\n",
    "\n",
    "Data Preprocessing Mistake: During data preprocessing, you accidentally include \"outstanding debt\" as a feature in your model, not realizing that this information is not available at the time of loan application. In other words, the model has access to information about an applicant's outstanding debt before they even apply for a loan.\n",
    "\n",
    "Model Training: You train a machine learning model (e.g., logistic regression) on this dataset, aiming to predict loan defaults based on the features, including \"outstanding debt.\"\n",
    "\n",
    "Problem: The model achieves excellent performance during cross-validation because it effectively \"cheats\" by using information about the applicant's outstanding debt, which is not available when making real loan approval decisions. In practice, the model is useless because it relies on data leakage, and it won't perform well on new loan applications.\n",
    "\n",
    "In this example, data leakage occurred because the model was trained using information that would not be available in a real-world scenario. To avoid data leakage, it's crucial to carefully preprocess the data, ensure that only information available at the time of prediction is used, and maintain a clear separation between training and testing data. Data leakage can be prevented through rigorous data preprocessing and validation procedures, as well as a deep understanding of the problem domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98510f46",
   "metadata": {},
   "source": [
    "#### Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c34816",
   "metadata": {},
   "source": [
    "To avoid data leakage, it's crucial to carefully preprocess the data, ensure that only information available at the time of prediction is used, and maintain a clear separation between training and testing data. Data leakage can be prevented through rigorous data preprocessing and validation procedures, as well as a deep understanding of the problem domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f9afe7",
   "metadata": {},
   "source": [
    "#### Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b04bef",
   "metadata": {},
   "source": [
    "A confusion matrix is a fundamental tool for evaluating the performance of a classification model, especially in machine learning tasks where you have a binary (two-class) or multiclass classification problem. It provides a clear and detailed breakdown of the model's predictions and their agreement with the actual class labels. A confusion matrix is often used to compute various classification metrics and gain insights into how well the model is performing.\n",
    "\n",
    "Here's what a confusion matrix tells you about the performance of a classification model:\n",
    "\n",
    "True Positives (TP): These are instances correctly predicted as the positive class.\n",
    "\n",
    "True Negatives (TN): These are instances correctly predicted as the negative class.\n",
    "\n",
    "False Positives (FP): These are instances incorrectly predicted as the positive class when they actually belong to the negative class. Also known as Type I errors.\n",
    "\n",
    "False Negatives (FN): These are instances incorrectly predicted as the negative class when they actually belong to the positive class. Also known as Type II errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64831a49",
   "metadata": {},
   "source": [
    "#### Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d76021f",
   "metadata": {},
   "source": [
    "**Precision** and **recall** are two important metrics in the context of a confusion matrix, and they provide complementary insights into the performance of a classification model. They are particularly useful when dealing with imbalanced datasets or when different types of errors have varying costs or consequences. Here's an explanation of the difference between precision and recall:\n",
    "\n",
    "1. **Precision:**\n",
    "   \n",
    "   - **Definition:** Precision measures the model's ability to correctly identify positive instances among all instances it predicted as positive. In other words, it quantifies the accuracy of the positive predictions.\n",
    "   \n",
    "   - **Formula:** Precision is calculated as \\(\\frac{TP}{TP + FP}\\), where TP represents true positives (correctly predicted positive instances), and FP represents false positives (instances predicted as positive but are actually negative).\n",
    "\n",
    "   - **Interpretation:** A high precision score indicates that when the model predicts a positive class, it is highly likely to be correct. It reflects the model's ability to avoid making false positive errors.\n",
    "\n",
    "   - **Use Case:** Precision is valuable when the cost or consequences of false positive errors are high. For example, in a medical diagnosis task, you want to ensure that when the model predicts a disease, it's highly accurate to avoid unnecessary treatments or anxiety for patients.\n",
    "\n",
    "2. **Recall (Sensitivity or True Positive Rate):**\n",
    "   \n",
    "   - **Definition:** Recall measures the model's ability to capture all positive instances correctly. It quantifies the ability of the model to find all relevant positive instances.\n",
    "   \n",
    "   - **Formula:** Recall is calculated as \\(\\frac{TP}{TP + FN}\\), where TP represents true positives (correctly predicted positive instances), and FN represents false negatives (instances predicted as negative but are actually positive).\n",
    "\n",
    "   - **Interpretation:** A high recall score indicates that the model is effective at identifying most of the positive instances in the dataset. It reflects the model's ability to minimize false negative errors.\n",
    "\n",
    "   - **Use Case:** Recall is crucial when missing positive instances has significant consequences or costs. For example, in a spam email filter, you want to ensure that the filter identifies as many spam emails as possible to protect users from unwanted messages.\n",
    "\n",
    "In summary, precision and recall offer different perspectives on a classification model's performance:\n",
    "\n",
    "- Precision focuses on the accuracy of positive predictions and is concerned with minimizing false positive errors. It answers the question: \"Of all instances predicted as positive, how many are truly positive?\"\n",
    "\n",
    "- Recall focuses on the model's ability to capture all positive instances and is concerned with minimizing false negative errors. It answers the question: \"Of all truly positive instances, how many did the model correctly identify?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c2d9f0",
   "metadata": {},
   "source": [
    "#### Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68737ae0",
   "metadata": {},
   "source": [
    "Here's how to interpret the confusion matrix:\n",
    "\n",
    "True Positives (TP): These are instances that the model correctly predicted as the positive class. For example, in a medical diagnosis task, these are patients with the disease who were correctly identified as having the disease.\n",
    "\n",
    "True Negatives (TN): These are instances that the model correctly predicted as the negative class. In a medical context, these could be healthy patients correctly identified as not having the disease.\n",
    "\n",
    "False Positives (FP): These are instances that the model incorrectly predicted as the positive class when they actually belong to the negative class. False positives are also known as Type I errors. In a medical context, these could be healthy patients incorrectly diagnosed with the disease.\n",
    "\n",
    "False Negatives (FN): These are instances that the model incorrectly predicted as the negative class when they actually belong to the positive class. False negatives are also known as Type II errors. In a medical context, these could be patients with the disease who were incorrectly classified as healthy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2793d7",
   "metadata": {},
   "source": [
    "#### Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d619b49d",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "\n",
    "Definition: Accuracy measures the overall correctness of the model's predictions and is calculated as \n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "TP+TN+FP+FN\n",
    "TP+TN\n",
    "​\n",
    " .\n",
    "Precision:\n",
    "\n",
    "Definition: Precision measures the accuracy of positive predictions and is calculated as \n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "TP+FP\n",
    "TP\n",
    "​\n",
    " .\n",
    "Recall (Sensitivity or True Positive Rate):\n",
    "\n",
    "Definition: Recall measures the model's ability to capture all positive instances and is calculated as \n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "TP+FN\n",
    "TP\n",
    "​\n",
    " .\n",
    "Specificity (True Negative Rate):\n",
    "\n",
    "Definition: Specificity measures the model's ability to capture all negative instances and is calculated as \n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "TN+FP\n",
    "TN\n",
    "​\n",
    " .\n",
    "F1-Score:\n",
    "\n",
    "Definition: The F1-score is the harmonic mean of precision and recall and is often used when there is an imbalance between classes. It is calculated as \n",
    "2\n",
    "⋅\n",
    "Precision\n",
    "⋅\n",
    "Recall\n",
    "Precision\n",
    "+\n",
    "Recall\n",
    "2⋅ \n",
    "Precision+Recall\n",
    "Precision⋅Recall\n",
    "​\n",
    " .\n",
    "False Positive Rate (FPR):\n",
    "\n",
    "Definition: FPR measures the rate of false positive predictions and is calculated as \n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "FP+TN\n",
    "FP\n",
    "​\n",
    " .\n",
    "False Negative Rate (FNR):\n",
    "\n",
    "Definition: FNR measures the rate of false negative predictions and is calculated as \n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "FN+TP\n",
    "FN\n",
    "​\n",
    " .\n",
    "Positive Predictive Value (PPV):\n",
    "\n",
    "Definition: PPV is another term for precision and is calculated as \n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "TP+FP\n",
    "TP\n",
    "​\n",
    " .\n",
    "Negative Predictive Value (NPV):\n",
    "\n",
    "Definition: NPV is the complement of the false negative rate and is calculated as \n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "TN+FN\n",
    "TN\n",
    "​\n",
    " .\n",
    "Balanced Accuracy:\n",
    "\n",
    "Definition: Balanced accuracy considers both sensitivity (recall) and specificity and is calculated as \n",
    "Sensitivity\n",
    "+\n",
    "Specificity\n",
    "2\n",
    "2\n",
    "Sensitivity+Specificity\n",
    "​\n",
    " .\n",
    "For multiclass classification problems, you can extend these metrics to each class, compute macro and micro averages, and use additional metrics such as the confusion matrix for multiclass scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f12033f",
   "metadata": {},
   "source": [
    "#### Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d36de",
   "metadata": {},
   "source": [
    "The relationship between the accuracy of a classification model and the values in its confusion matrix is straightforward but important to understand. Accuracy is one of the performance metrics calculated based on the values in the confusion matrix, and it provides an overall measure of how well the model is performing. Here's how accuracy is related to the confusion matrix:\n",
    "\n",
    "In a binary classification confusion matrix:\n",
    "\n",
    "```\n",
    "                  Actual Class 0       Actual Class 1\n",
    "Predicted\n",
    "Class 0     True Negative (TN)      False Positive (FP)\n",
    "Class 1     False Negative (FN)     True Positive (TP)\n",
    "```\n",
    "\n",
    "Accuracy is calculated as:\n",
    "\n",
    "\\[ \\text{Accuracy} = \\frac{\\text{Correct Predictions (TP + TN)}}{\\text{Total Predictions (TP + TN + FP + FN)}} \\]\n",
    "\n",
    "In other words, accuracy measures the proportion of correctly predicted instances (both true negatives and true positives) out of all instances the model made predictions for (the total number of predictions).\n",
    "\n",
    "Here's how accuracy relates to the values in the confusion matrix:\n",
    "\n",
    "- **True Positives (TP):** These are instances correctly predicted as the positive class. They contribute positively to accuracy.\n",
    "\n",
    "- **True Negatives (TN):** These are instances correctly predicted as the negative class. They also contribute positively to accuracy.\n",
    "\n",
    "- **False Positives (FP):** These are instances incorrectly predicted as the positive class when they are actually negative. They contribute negatively to accuracy.\n",
    "\n",
    "- **False Negatives (FN):** These are instances incorrectly predicted as the negative class when they are actually positive. They also contribute negatively to accuracy.\n",
    "\n",
    "In summary:\n",
    "\n",
    "- **Accuracy** measures the overall correctness of the model's predictions.\n",
    "\n",
    "- **True Positives** and **True Negatives** contribute positively to accuracy.\n",
    "\n",
    "- **False Positives** and **False Negatives** contribute negatively to accuracy.\n",
    "\n",
    "Accuracy provides a global view of how well the model is performing in terms of making correct predictions, but it may not be the best metric in all scenarios, especially when dealing with imbalanced datasets or when different types of errors have varying consequences. Therefore, it's important to consider additional performance metrics such as precision, recall, F1-score, and others, depending on the specific goals and requirements of your classification task. These metrics provide a more nuanced understanding of the model's performance and its ability to balance between different types of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c453e87",
   "metadata": {},
   "source": [
    "#### Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9278db",
   "metadata": {},
   "source": [
    "A confusion matrix can be a valuable tool for identifying potential biases or limitations in your machine learning model, especially when you are concerned about issues related to bias, fairness, and model performance disparities across different groups. Here's how you can use a confusion matrix to uncover such biases or limitations:\n",
    "\n",
    "1. **Class Imbalance:** Examine the distribution of actual class labels in the confusion matrix. If one class vastly outnumbers the other, it may indicate a class imbalance issue. Biases may emerge if the model tends to favor the majority class and performs poorly on the minority class. This is particularly important in scenarios where both classes are critical, such as medical diagnoses or fraud detection.\n",
    "\n",
    "2. **False Positives and False Negatives:** Analyze the rates of false positives (FP) and false negatives (FN) in the confusion matrix, especially in binary classification problems. Determine whether one type of error is more frequent or has more severe consequences than the other. Depending on your application, false positives or false negatives may be more problematic, and a high rate of either can indicate a limitation in your model.\n",
    "\n",
    "3. **Demographic Disparities:** If your dataset includes demographic information (e.g., gender, race, age), break down the confusion matrix by these demographics to identify disparities. Check whether certain groups experience higher false positive or false negative rates. Biases may emerge if the model performs differently for different demographic groups, potentially leading to unfair outcomes.\n",
    "\n",
    "4. **Threshold Effects:** Adjusting the classification threshold can have a significant impact on the model's behavior. By examining the trade-offs between precision and recall at different thresholds, you can identify how the model's performance varies. Biases or limitations may become apparent if changing the threshold disproportionately affects certain groups or types of errors.\n",
    "\n",
    "5. **Fairness Metrics:** Consider using fairness metrics in addition to traditional performance metrics. Metrics like disparate impact, equal opportunity, and demographic parity can provide quantitative measures of fairness and help identify disparities in how the model treats different groups.\n",
    "\n",
    "6. **Bias Mitigation:** If you identify biases or limitations in your model, take steps to mitigate them. This may involve reevaluating your data collection process, using fairness-aware algorithms, adjusting the model's training objectives, or implementing post-processing techniques to mitigate biases in predictions.\n",
    "\n",
    "7. **Feedback Loops:** Continuously monitor the model's performance and its potential biases in a production environment. Collect feedback from users or domain experts and use it to refine the model over time. Bias and fairness considerations should be an ongoing part of model development and maintenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932e69a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
