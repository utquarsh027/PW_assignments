{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b077afb3",
   "metadata": {},
   "source": [
    "#### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03473e12",
   "metadata": {},
   "source": [
    "Min-max scaling is a normalization scaling technique that scale the features in the range of 0 and 1\n",
    "- x_new=(x-x_min)/(x-max-x_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "477bfe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a3e7ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c93a33fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  \n",
       "2  woman       False  NaN  Southampton   yes   True  \n",
       "3  woman       False    C  Southampton   yes  False  \n",
       "4    man        True  NaN  Southampton    no   True  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dbaffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e49e9c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86c47849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(df[['fare']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71da4620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.014151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.139136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.103644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0.025374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>0.058556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0.045771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>0.058556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0.015127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     fare_scaled\n",
       "0       0.014151\n",
       "1       0.139136\n",
       "2       0.015469\n",
       "3       0.103644\n",
       "4       0.015713\n",
       "..           ...\n",
       "886     0.025374\n",
       "887     0.058556\n",
       "888     0.045771\n",
       "889     0.058556\n",
       "890     0.015127\n",
       "\n",
       "[891 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scaler.transform(df[['fare']]),columns=['fare_scaled'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eac7a32",
   "metadata": {},
   "source": [
    "#### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaa830f",
   "metadata": {},
   "source": [
    "Unit vector is a type of preprocessing technique in ML to standarize or normalize the range of independent variable or features in a dataset.\n",
    "In this we scale the feature in a way that it represents the direction whereas in MinMax scaler we scale the feature in the range 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e55393f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a594a66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09ee0ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width',\n",
       "       'species'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b183abc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88b1ff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=normalize(df1[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa7aa882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.803773</td>\n",
       "      <td>0.551609</td>\n",
       "      <td>0.220644</td>\n",
       "      <td>0.031521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.828133</td>\n",
       "      <td>0.507020</td>\n",
       "      <td>0.236609</td>\n",
       "      <td>0.033801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.805333</td>\n",
       "      <td>0.548312</td>\n",
       "      <td>0.222752</td>\n",
       "      <td>0.034269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.800030</td>\n",
       "      <td>0.539151</td>\n",
       "      <td>0.260879</td>\n",
       "      <td>0.034784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.790965</td>\n",
       "      <td>0.569495</td>\n",
       "      <td>0.221470</td>\n",
       "      <td>0.031639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.721557</td>\n",
       "      <td>0.323085</td>\n",
       "      <td>0.560015</td>\n",
       "      <td>0.247699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.729654</td>\n",
       "      <td>0.289545</td>\n",
       "      <td>0.579090</td>\n",
       "      <td>0.220054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.716539</td>\n",
       "      <td>0.330710</td>\n",
       "      <td>0.573231</td>\n",
       "      <td>0.220474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.674671</td>\n",
       "      <td>0.369981</td>\n",
       "      <td>0.587616</td>\n",
       "      <td>0.250281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.690259</td>\n",
       "      <td>0.350979</td>\n",
       "      <td>0.596665</td>\n",
       "      <td>0.210588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width\n",
       "0        0.803773     0.551609      0.220644     0.031521\n",
       "1        0.828133     0.507020      0.236609     0.033801\n",
       "2        0.805333     0.548312      0.222752     0.034269\n",
       "3        0.800030     0.539151      0.260879     0.034784\n",
       "4        0.790965     0.569495      0.221470     0.031639\n",
       "..            ...          ...           ...          ...\n",
       "145      0.721557     0.323085      0.560015     0.247699\n",
       "146      0.729654     0.289545      0.579090     0.220054\n",
       "147      0.716539     0.330710      0.573231     0.220474\n",
       "148      0.674671     0.369981      0.587616     0.250281\n",
       "149      0.690259     0.350979      0.596665     0.210588\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scaler,columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a81e9e",
   "metadata": {},
   "source": [
    "#### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328316f0",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) and feature extraction are closely related concepts in the field of dimensionality reduction and machine learning. PCA can indeed be used as a method for feature extraction. Let's delve into the relationship and illustrate it with an example:\n",
    "\n",
    "**Relationship between PCA and Feature Extraction:**\n",
    "- **PCA**: PCA is a technique primarily used for dimensionality reduction. It identifies the most important patterns or directions (principal components) in the data and projects the data onto these components, effectively reducing the number of dimensions while retaining as much variance as possible.\n",
    "- **Feature Extraction**: Feature extraction is the process of transforming the original features into a new set of features that captures the most important information in the data. This can help improve the efficiency and effectiveness of machine learning algorithms by focusing on the most relevant aspects of the data.\n",
    "\n",
    "**Using PCA for Feature Extraction:**\n",
    "PCA can be used as a feature extraction technique. By identifying the principal components, PCA essentially creates a new set of features that are linear combinations of the original features. These new features capture the most significant variations in the data, often representing high-level patterns that can be useful for downstream tasks like classification or regression.\n",
    "\n",
    "**Example: Iris Flower Classification:**\n",
    "Let's consider the classic Iris flower dataset, which contains measurements of sepal length, sepal width, petal length, and petal width for different iris flower species.\n",
    "\n",
    "1. **Original Features**: You have four original features: sepal length, sepal width, petal length, and petal width.\n",
    "\n",
    "2. **Applying PCA for Feature Extraction**:\n",
    "   - Standardize the data.\n",
    "   - Calculate the covariance matrix and its eigenvectors and eigenvalues.\n",
    "   - Sort the eigenvectors based on eigenvalues to identify the most important principal components.\n",
    "\n",
    "3. **Selecting Principal Components**:\n",
    "   - Let's say you decide to retain the top two principal components, which explain most of the variance in the data.\n",
    "\n",
    "4. **Creating New Features**:\n",
    "   - Project the original data onto the two selected principal components. This creates two new features that are linear combinations of the original features.\n",
    "\n",
    "5. **Using Extracted Features**:\n",
    "   - These two new features are your extracted features. They represent combinations of the original features that capture the most significant variations in the data.\n",
    "   - You can now use these two extracted features as input for a machine learning algorithm, like a classifier.\n",
    "\n",
    "In this example, PCA has effectively transformed the original features into a reduced set of features that still retain the essence of the data. These extracted features might prove more effective for classification tasks compared to using all the original features, especially if there was multicollinearity or a high number of dimensions.\n",
    "\n",
    "In summary, PCA can be used as a tool for feature extraction by creating new features that represent the most important patterns in the data. This can lead to more efficient and accurate machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfad8e0",
   "metadata": {},
   "source": [
    "#### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f843b8",
   "metadata": {},
   "source": [
    "WCertainly! Min-Max scaling is a method of feature scaling that transforms the features in a dataset to a specific range, typically between 0 and 1. This can be useful when you want to ensure that different features with different scales are comparable and don't overly influence the recommendations made by your system. Let's go through the steps of how you could use Min-Max scaling to preprocess the features in your food delivery recommendation system:\n",
    "\n",
    "**Step 1: Understanding the Features**\n",
    "First, you need to understand the features in your dataset. In your case, you have features like price, rating, and delivery time. These features might have different ranges and units, which can make it hard for your recommendation system to make fair comparisons.\n",
    "\n",
    "**Step 2: Identify the Ranges**\n",
    "Find out the minimum and maximum values for each feature. For example, the price might range from $5 to $50, the ratings might go from 1 to 5, and delivery time might range from 10 minutes to 60 minutes.\n",
    "\n",
    "**Step 3: Applying Min-Max Scaling**\n",
    "Now, apply Min-Max scaling to each feature. The formula to transform a value \\(x\\) to its scaled value \\(x_{\\text{scaled}}\\) is:\n",
    "\n",
    "\\[ x_{\\text{scaled}} = \\frac{x - \\text{min}(x)}{\\text{max}(x) - \\text{min}(x)} \\]\n",
    "\n",
    "Where:\n",
    "- (x_scaled) is the scaled value of the feature.\n",
    "- (x) is the original value of the feature.\n",
    "- (min(x)) is the minimum value of the feature.\n",
    "- (max(x)) is the maximum value of the feature.\n",
    "\n",
    "Apply this formula to each value of each feature in your dataset.\n",
    "\n",
    "**Step 4: Interpretation of Scaled Values**\n",
    "After applying Min-Max scaling, the values of all your features will now fall between 0 and 1. A value of 0 means the minimum value of that feature, and a value of 1 means the maximum value of that feature. The scaled values now allow for fair comparisons between the features.\n",
    "\n",
    "**Step 5: Using the Scaled Data for Recommendations**\n",
    "With your features scaled between 0 and 1, your recommendation system can now work more effectively. It can weigh each feature equally without one feature dominating the others due to its larger range.\n",
    "\n",
    "For example, if a user is concerned about both price and delivery time, the system can give equal weight to both features, as they are now on the same scale. The system can analyze and compare the scaled values to make personalized food recommendations based on the user's preferences.\n",
    "\n",
    "In summary, Min-Max scaling transforms your feature values to a common range, making them comparable and helping your recommendation system make better suggestions by treating all features equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed78ae57",
   "metadata": {},
   "source": [
    "#### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de5505a",
   "metadata": {},
   "source": [
    "Sure, I'd be happy to explain how you could use Principal Component Analysis (PCA) to reduce the dimensionality of your stock price prediction dataset:\n",
    "\n",
    "**Step 1: Understanding Dimensionality Reduction:**\n",
    "In your dataset, you likely have many features (dimensions) that describe different aspects of stock data. However, having a large number of features can lead to complexity, computational inefficiency, and potentially overfitting your model.\n",
    "\n",
    "**Step 2: Applying PCA:**\n",
    "PCA is a technique used to reduce the dimensionality of a dataset while preserving as much variance (information) as possible. Here's how you could apply PCA to your stock price prediction dataset:\n",
    "\n",
    "1. **Standardize the Data:**\n",
    "   Start by standardizing your features, which means making them have a mean of 0 and a standard deviation of 1. This is important because PCA is sensitive to the scale of the features.\n",
    "\n",
    "2. **Compute the Covariance Matrix:**\n",
    "   Calculate the covariance matrix of the standardized data. This matrix shows how the features vary together.\n",
    "\n",
    "3. **Compute Eigenvectors and Eigenvalues:**\n",
    "   Find the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent directions of maximum variance, and eigenvalues indicate the amount of variance explained by each eigenvector.\n",
    "\n",
    "4. **Sort Eigenvectors by Eigenvalues:**\n",
    "   Sort the eigenvectors in descending order based on their corresponding eigenvalues. This step helps you identify the most important directions (principal components) that capture the most variance in the data.\n",
    "\n",
    "5. **Select Principal Components:**\n",
    "   Choose the top \\(k\\) eigenvectors that correspond to the highest eigenvalues. These principal components represent the most important patterns in your data.\n",
    "\n",
    "6. **Project Data onto Lower-Dimensional Space:**\n",
    "   Create a new matrix by projecting your original data onto the selected principal components. This effectively reduces the dimensionality of your dataset from the original number of features to \\(k\\) features (where \\(k\\) is the number of principal components you selected).\n",
    "\n",
    "**Step 3: Benefits of PCA:**\n",
    "Using PCA to reduce dimensionality offers several advantages:\n",
    "\n",
    "- **Simplification:** PCA simplifies your dataset by focusing on the most important features.\n",
    "- **Noise Reduction:** It can help remove noise and unimportant variations in the data.\n",
    "- **Computation Efficiency:** Working with fewer dimensions speeds up computation.\n",
    "- **Visualization:** Lower-dimensional data is easier to visualize and interpret.\n",
    "- **Overfitting Prevention:** Reducing dimensions can help prevent overfitting by reducing model complexity.\n",
    "\n",
    "**Step 4: Using Reduced Data for Modeling:**\n",
    "You can now use the lower-dimensional data obtained through PCA as input for your stock price prediction model. It's important to note that while PCA reduces dimensionality, it doesn't always guarantee better performance. It's a trade-off between simplicity and retaining as much useful information as possible.\n",
    "\n",
    "In summary, PCA is a powerful technique to reduce the complexity of your dataset while preserving important patterns. It can help your stock price prediction model perform better by focusing on the most relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c7a7b",
   "metadata": {},
   "source": [
    "#### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec77196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.DataFrame([1, 5, 10, 15, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d63c3ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a22d074d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.21052632],\n",
       "       [0.47368421],\n",
       "       [0.73684211],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit_transform(df2[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72d9a56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
